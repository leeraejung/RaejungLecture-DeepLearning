{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be8459af",
   "metadata": {},
   "source": [
    "# 신경망 학습 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c93566",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f758a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rc('figure', figsize=(10, 6))\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.family'] = 'New Gulim'\n",
    "rcParams['font.size'] = 10\n",
    "rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f3855a",
   "metadata": {},
   "source": [
    "# 1 매개변수 갱신 - Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0abdba7",
   "metadata": {},
   "source": [
    "### 1.1 확률적 경사 하강법 - SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66568f9b",
   "metadata": {},
   "source": [
    "<img src=\"./images/e_6.1.png\" width=\"180\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f861d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD 클래스 구현\n",
    "class SGD:\n",
    "    \"\"\"확률적 경사 하강법（Stochastic Gradient Descent）\"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae62a71",
   "metadata": {},
   "source": [
    "### 1.2 SGD의 단점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63973064",
   "metadata": {},
   "source": [
    "- 함수 정의\n",
    "<img src=\"./images/e_6.2.png\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3b9e19",
   "metadata": {},
   "source": [
    "- 그래프와 등고선\n",
    "<img src=\"./images/fig_6-1.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ac559",
   "metadata": {},
   "source": [
    "- 함수의 기울기\n",
    "<img src=\"./images/fig_6-2.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31827e53",
   "metadata": {},
   "source": [
    "- SGD에 의한 최적화 갱신 경로: 초기값 (-7.0, 2.0)\n",
    "<img src=\"./images/fig_6-3.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f39006c",
   "metadata": {},
   "source": [
    "### 1.3 모멘텀 - Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaaa3c5",
   "metadata": {},
   "source": [
    "<img src=\"./images/e_6.3.png\" width=\"180\"/>\n",
    "<img src=\"./images/e_6.4.png\" width=\"150\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61828f2",
   "metadata": {},
   "source": [
    "- 모멘텀의 이미지\n",
    "<img src=\"./images/fig_6-4.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e09f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모멘텀 클래스 구현\n",
    "class Momentum:\n",
    "    \"\"\"모멘텀 SGD\"\"\"\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():                                \n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n",
    "            params[key] += self.v[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fff9d0",
   "metadata": {},
   "source": [
    "- 모멘텀에 의한 최적화 갱신 경로: 초기값 (-7.0, 2.0)\n",
    "<img src=\"./images/fig_6-5.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04026c93",
   "metadata": {},
   "source": [
    "### 1.4 AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d75d31",
   "metadata": {},
   "source": [
    "<img src=\"./images/e_6.5.png\" width=\"180\"/>\n",
    "<img src=\"./images/e_6.6.png\" width=\"180\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae609621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaGrad 클래스 구현\n",
    "class AdaGrad:\n",
    "    \"\"\"AdaGrad\"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5309e88a",
   "metadata": {},
   "source": [
    "- AdaGrad에 의한 최적화 갱신 경로: 초기값 (-7.0, 2.0)\n",
    "<img src=\"./images/fig_6-6.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad6efa",
   "metadata": {},
   "source": [
    "### 1.5 Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47c2b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc96e66d",
   "metadata": {},
   "source": [
    "- Adam에 의한 최적화 갱신 경로: 초기값 (-7.0, 2.0)\n",
    "<img src=\"./images/fig_6-7.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2135e7a3",
   "metadata": {},
   "source": [
    "### 1.6 Optimizer 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb690077",
   "metadata": {},
   "source": [
    "#### 1.6.1 다변수 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff13603c",
   "metadata": {},
   "source": [
    "- 함수\n",
    "<img src=\"./images/e_6.2.png\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7038fc6a",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_6-8.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d882b98",
   "metadata": {},
   "source": [
    "- 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca94204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from common.optimizer import *\n",
    "\n",
    "# 함수 정의\n",
    "def f(x, y):\n",
    "    return x**2 / 20.0 + y**2\n",
    "\n",
    "# 도함수 정의\n",
    "def df(x, y):\n",
    "    return x / 10.0, 2.0*y\n",
    "\n",
    "# 초기값\n",
    "init_pos = (-7.0, 2.0)\n",
    "params = {}\n",
    "params['x'], params['y'] = init_pos[0], init_pos[1]\n",
    "grads = {}\n",
    "grads['x'], grads['y'] = 0, 0\n",
    "\n",
    "optimizers = OrderedDict()\n",
    "optimizers[\"SGD\"] = SGD(lr=0.95)\n",
    "optimizers[\"Momentum\"] = Momentum(lr=0.1)\n",
    "optimizers[\"AdaGrad\"] = AdaGrad(lr=1.5)\n",
    "optimizers[\"Adam\"] = Adam(lr=0.3)\n",
    "\n",
    "idx = 1\n",
    "\n",
    "for key in optimizers:\n",
    "    optimizer = optimizers[key]\n",
    "    x_history = []\n",
    "    y_history = []\n",
    "    params['x'], params['y'] = init_pos[0], init_pos[1]\n",
    "    \n",
    "    for i in range(30):\n",
    "        x_history.append(params['x'])\n",
    "        y_history.append(params['y'])\n",
    "        \n",
    "        grads['x'], grads['y'] = df(params['x'], params['y'])\n",
    "        optimizer.update(params, grads)\n",
    "    \n",
    "\n",
    "    x = np.arange(-10, 10, 0.01)\n",
    "    y = np.arange(-5, 5, 0.01)\n",
    "    \n",
    "    X, Y = np.meshgrid(x, y) \n",
    "    Z = f(X, Y)\n",
    "    \n",
    "    # 외곽선 단순화\n",
    "    mask = Z > 7\n",
    "    Z[mask] = 0\n",
    "    \n",
    "    # 그래프 그리기\n",
    "    plt.subplot(2, 2, idx)\n",
    "    idx += 1\n",
    "    plt.plot(x_history, y_history, 'o-', color=\"red\")\n",
    "    plt.contour(X, Y, Z)\n",
    "    plt.ylim(-10, 10)\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.plot(0, 0, '+')\n",
    "    #colorbar()\n",
    "    #spring()\n",
    "    plt.title(key)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4e9f11",
   "metadata": {},
   "source": [
    "#### 1.6.2  MNIST 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c17cca9",
   "metadata": {},
   "source": [
    "- MNIST 데이터셋 학습 진행 과정\n",
    "<img src=\"./images/fig_6-9.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f03c637",
   "metadata": {},
   "source": [
    "- MNIST 학습 진행 과정 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edb8737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from dataset.mnist import load_mnist\n",
    "from common.util import smooth_curve\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import *\n",
    "\n",
    "# 0. MNIST 데이터 읽기==========\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 128\n",
    "max_iterations = 2000\n",
    "\n",
    "# 1. 실험용 설정==========\n",
    "optimizers = {}\n",
    "optimizers['SGD'] = SGD()\n",
    "optimizers['Momentum'] = Momentum()\n",
    "optimizers['AdaGrad'] = AdaGrad()\n",
    "optimizers['Adam'] = Adam()\n",
    "#optimizers['RMSprop'] = RMSprop()\n",
    "\n",
    "networks = {}\n",
    "train_loss = {}\n",
    "for key in optimizers.keys():\n",
    "    networks[key] = MultiLayerNet(\n",
    "        input_size=784, hidden_size_list=[100, 100, 100, 100],\n",
    "        output_size=10)\n",
    "    train_loss[key] = []    \n",
    "\n",
    "# 2. 훈련 시작==========\n",
    "for i in range(max_iterations):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    for key in optimizers.keys():\n",
    "        grads = networks[key].gradient(x_batch, t_batch)\n",
    "        optimizers[key].update(networks[key].params, grads)\n",
    "    \n",
    "        loss = networks[key].loss(x_batch, t_batch)\n",
    "        train_loss[key].append(loss)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print( \"===========\" + \"iteration:\" + str(i) + \"===========\")\n",
    "        for key in optimizers.keys():\n",
    "            loss = networks[key].loss(x_batch, t_batch)\n",
    "            print(key + \":\" + str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3b7070",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3. 그래프 그리기==========\n",
    "markers = {\"SGD\": \"o\", \"Momentum\": \"x\", \"AdaGrad\": \"s\", \"Adam\": \"D\"}\n",
    "x = np.arange(max_iterations)\n",
    "for key in optimizers.keys():\n",
    "    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c7eb8a",
   "metadata": {},
   "source": [
    "# 2 가중치 초기값\n",
    "- sigmoid, tanh: Xavier 초기값\n",
    "- ReLU:  He 초기값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63816d45",
   "metadata": {},
   "source": [
    "### 2.1 은닉층의 활성화값 분포\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858f9244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc462d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a9972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f98db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activation( w_init_case = 1, act_func = sigmoid):\n",
    "    input_data = np.random.randn(1000, 100)  # 1000개의 데이터\n",
    "    node_num = 100                           # 각 은닉층의 노드(뉴런) 수\n",
    "    hidden_layer_size = 5                    # 은닉층이 5개\n",
    "    activations = {}                         # 이곳에 활성화 결과를 저장\n",
    "\n",
    "    x = input_data\n",
    "\n",
    "    for i in range(hidden_layer_size):\n",
    "        if i != 0:\n",
    "            x = activations[i-1]\n",
    "\n",
    "        # 초깃값을 다양하게 바꿔가며 실험해보자！\n",
    "        if w_init_case   == 1:\n",
    "            w = np.random.randn(node_num, node_num) * 1\n",
    "        elif w_init_case == 2:\n",
    "            w = np.random.randn(node_num, node_num) * 0.01\n",
    "        elif w_init_case == 3:\n",
    "            w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
    "        elif w_init_case == 4:\n",
    "            w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
    "        else:\n",
    "            w = np.random.randn(node_num, node_num) * 1\n",
    "\n",
    "        a = np.dot(x, w)\n",
    "\n",
    "        # 활성화 함수도 바꿔가며 실험해보자！\n",
    "        z = act_func(a)\n",
    "        # z = sigmoid(a)\n",
    "        # z = ReLU(a)\n",
    "        # z = tanh(a)\n",
    "\n",
    "        activations[i] = z\n",
    "\n",
    "    # 히스토그램 그리기\n",
    "    for i, a in activations.items():\n",
    "        plt.subplot(1, len(activations), i+1)\n",
    "        plt.title(str(i+1) + '-layer')\n",
    "        if i != 0: plt.yticks([], [])\n",
    "        plt.hist(a.flatten(), 30, range=(0,1))\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cae9aff",
   "metadata": {},
   "source": [
    "#### 2.1.1 가중치를 표준편차가 1인 정규분포로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e5057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w: 표준편차 1인 정규분포로 초기화, sigmoid 사용\n",
    "plot_activation(w_init_case=1, act_func=sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193d0b87",
   "metadata": {},
   "source": [
    "#### 2.1.2 가중치를 표준편차가 0.01인 정규분포로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b6ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w: 표준편차 0.01인 정규분포로 초기화, sigmoid 사용\n",
    "plot_activation(w_init_case=2, act_func=sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1934fddd",
   "metadata": {},
   "source": [
    "#### 2.1.3 Xavier 초기값\n",
    "- 사비에르 글로로트(Xavier Glorot), 요슈아 벤지오(Yoshua Bengio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1150bfa5",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig_6-12.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831191e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w: Xavier 초기화, sigmoid 사용\n",
    "plot_activation(w_init_case=3, act_func=sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937abfb9",
   "metadata": {},
   "source": [
    "#### 2.1.4 He 초기값\n",
    "- 카이밍 히(Kaîming He)\n",
    "- ReLU 사용시 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28666f4",
   "metadata": {},
   "source": [
    "- w: 표준편차가 $\\sqrt{\\frac{2}{n}}$ 인 정규 분포로 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5f177e",
   "metadata": {},
   "source": [
    "#### ReLU 사용시 가중치 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a34dbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w: 표준편차 0.01인 정규분포로 초기화, ReLU 사용\n",
    "plot_activation(w_init_case=2, act_func=ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca36e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w: Xavier 초기화, ReLU 사용\n",
    "plot_activation(w_init_case=3, act_func=ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee04f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w: He 초기화, ReLU 사용\n",
    "plot_activation(w_init_case=4, act_func=ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb73411",
   "metadata": {},
   "source": [
    "### 2.2 가중치 초기값 비교: MNIST 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4d169d",
   "metadata": {},
   "source": [
    "- MNIST 데이터셋 학습 진행 과정: 기중치 초기값에 따른 비교\n",
    "<img src=\"./images/fig_6-15.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee3d282",
   "metadata": {},
   "source": [
    "- MNIST 학습 진행 과정 코드 구현: 기중치 초기값에 따른 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ef9222",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from dataset.mnist import load_mnist\n",
    "from common.util import smooth_curve\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import SGD\n",
    "\n",
    "# 0. MNIST 데이터 읽기==========\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 128\n",
    "max_iterations = 2000\n",
    "\n",
    "# 1. 실험용 설정==========\n",
    "weight_init_types = {'std=0.01': 0.01, 'Xavier': 'sigmoid', 'He': 'relu'}\n",
    "optimizer = SGD(lr=0.01)\n",
    "\n",
    "networks = {}\n",
    "train_loss = {}\n",
    "for key, weight_type in weight_init_types.items():\n",
    "    networks[key] = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100],\n",
    "                                  output_size=10, weight_init_std=weight_type)\n",
    "    train_loss[key] = []\n",
    "\n",
    "# 2. 훈련 시작==========\n",
    "for i in range(max_iterations):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    for key in weight_init_types.keys():\n",
    "        grads = networks[key].gradient(x_batch, t_batch)\n",
    "        optimizer.update(networks[key].params, grads)\n",
    "    \n",
    "        loss = networks[key].loss(x_batch, t_batch)\n",
    "        train_loss[key].append(loss)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"===========\" + \"iteration:\" + str(i) + \"===========\")\n",
    "        for key in weight_init_types.keys():\n",
    "            loss = networks[key].loss(x_batch, t_batch)\n",
    "            print(key + \":\" + str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2378740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 그래프 그리기==========\n",
    "markers = {'std=0.01': 'o', 'Xavier': 's', 'He': 'D'}\n",
    "x = np.arange(max_iterations)\n",
    "for key in weight_init_types.keys():\n",
    "    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(0, 2.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1f59e",
   "metadata": {},
   "source": [
    "# 3 배치 정규화 - Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d106744",
   "metadata": {},
   "source": [
    "### 3.1 배치 정규화 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b004b23",
   "metadata": {},
   "source": [
    "- 학습 속도 개선\n",
    "- 초기값 의존도 낮춤\n",
    "- 오버피팅 억제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3984c3f",
   "metadata": {},
   "source": [
    "- 배치 정규화를 사용한 신경망의 예\n",
    "<img src=\"./images/fig_6-16.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97379577",
   "metadata": {},
   "source": [
    "- 배치 정규화 수식\n",
    "<img src=\"./images/e_6.7.png\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104fba69",
   "metadata": {},
   "source": [
    "- 배치 정규화의 확대(scale), 이동(shift)\n",
    "- $\\gamma = 1, \\beta = 0$ 으로 학습 시작, 학습 진행에 따라서 조정\n",
    "<img src=\"./images/e_6.8.png\" width=\"150\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf574af",
   "metadata": {},
   "source": [
    "### 3.2 배치 정규화 효과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f389af",
   "metadata": {},
   "source": [
    "- 배치 정규화 효과: 학습 속도 향상\n",
    "<img src=\"./images/fig_6-18.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3972f8cf",
   "metadata": {},
   "source": [
    "- 가중치 초기값에 따른 학습 진행\n",
    "- 실선: 배치 정규화 사용, 점선: 배치 정규화 상용하지 않음\n",
    "<img src=\"./images/fig_6-19.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d84020",
   "metadata": {},
   "source": [
    "# 4 오버피팅 - Over fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2346c13",
   "metadata": {},
   "source": [
    "### 4.1 오버피팅된 학습 진행 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab65d4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import SGD\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# weight decay（가중치 감쇠） 설정 =========================\n",
    "weight_decay_lambda = 0 # weight decay를 사용하지 않을 경우\n",
    "#weight_decay_lambda = 0.1\n",
    "# ==========================================================\n",
    "\n",
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신\n",
    "\n",
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0\n",
    "\n",
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3726b2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 그리기==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.title('오버피팅된 학습 과정')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a189c5",
   "metadata": {},
   "source": [
    "### 4.2 가중치 감소 - L2 규제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41600890",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import SGD\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# weight decay（가중치 감쇠） 설정 =========================\n",
    "#weight_decay_lambda = 0 # weight decay를 사용하지 않을 경우\n",
    "weight_decay_lambda = 0.1\n",
    "# ==========================================================\n",
    "\n",
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda=weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신\n",
    "\n",
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0\n",
    "\n",
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215701a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 그리기==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.title('L2 규제 적용')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3eb9e9",
   "metadata": {},
   "source": [
    "### 4.3 드롭아웃 - Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba67e0d",
   "metadata": {},
   "source": [
    "- 뉴런을 무작위로 삭제하여 신호 전달을 차단\n",
    "<img src=\"./images/fig_6-22.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04697b8",
   "metadata": {},
   "source": [
    "#### 드롭아웃을 적용한 신경망 학습 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f6fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# 드롭아웃 사용 유무와 비울 설정 ========================\n",
    "use_dropout = True  # 드롭아웃을 쓰지 않을 때는 False\n",
    "dropout_ratio = 0.2\n",
    "dropout_ratio = 0.15\n",
    "# =======================================================\n",
    "\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e2916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 그리기==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.title('드롭아웃을 적용한 신경망 학습 과정: dropout_ratio = 0.15')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938f9df8",
   "metadata": {},
   "source": [
    "# 5 하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e7d753",
   "metadata": {},
   "source": [
    "- lr: 학습률\n",
    "- weight decay: L2 규제\n",
    "<img src=\"./images/fig_6-24.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bcaf62",
   "metadata": {},
   "source": [
    "- Best-1 (val acc:0.83) | lr:0.0092, weight decay: 3.86e-07\n",
    "- Best-2 (val acc:0.78) | lr:0.0095, weight decay: 6.04e-07"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46fba2a",
   "metadata": {},
   "source": [
    "# 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daf8f53",
   "metadata": {},
   "source": [
    "- 매개변수 갱신 방법에는 확률적 경사 하강법(SGD) 외에도 모멘텀, AdaGrad, Adam 등이 있다.\n",
    "- 가중치 초깃값을 정하는 방법은 올바른 학습을 히는 데 매우 중요하다.\n",
    "- 가중치의 초깃값으로는 'Xavier 초깃값'과 'He 초깃값'이 효괴적이다.\n",
    "- 배치 정규화를 이용하면 학습을 빠르게 진행할 수 있으며, 초기값에 영향을 덜 받게 된다.\n",
    "- 오버피팅을 억제하는 정규화 기술로는 기중치 감소와 드롭아웃이 있다.\n",
    "- 하이퍼따라미터 값 탐색은 최적 값이 존재할 법한 범위를 점차좁히면서 하는 것이 효과적이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd942b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f076fa13",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a4484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85afea49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vp": {
   "vp_config_version": "1.0.0",
   "vp_menu_width": 273,
   "vp_note_display": true,
   "vp_note_width": 263,
   "vp_position": {
    "width": 541
   },
   "vp_section_display": false,
   "vp_signature": "VisualPython"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
